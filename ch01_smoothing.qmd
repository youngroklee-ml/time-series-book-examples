# Smoothing

```{r}
library(tidyverse)
library(tsibble) # time series data frame wrangling
library(fable)   # forecasting models
library(feasts)  # feature extraction, statistics, visualization for time series data
library(readxl)
```

## Example 1.1

### Data

Create a data frame.

```{r}
forest_fire <- tribble(
  ~year, ~cnt,
  2006, 369,
  2007, 418,
  2008, 389,
  2009, 570,
  2010, 282,
  2011, 277,
  2012, 197,
  2013, 296,
  2014, 492,
  2015, 623,
  2016, 391
)
```

### Convert to time-series data

Convert the data frame to time series data frame (`tsibble`) object. You must set `index` with a column that represents timepoints for the series. Please note that `tsibble` automatically notice that the index is year and that the series is regular yearly series.

```{r}
forest_fire_ts <-
  forest_fire |>
  as_tsibble(index = year)

forest_fire_ts
```


### Moving average

Compute moving average by calling `slider::slide_mean()`. Set `complete = TRUE` to return missing value `NA` when there are missing observations in the sliding window.

```{r}
forest_fire_ma <-
  forest_fire_ts |>
  mutate(
    ma3 = slider::slide_mean(cnt, before = 2, after = 0, complete = TRUE),
    ma6 = slider::slide_mean(cnt, before = 5, after = 0, complete = TRUE)
  )

forest_fire_ma
```


:::{.callout-note}
`{slider}` package provides functions to conduct rolling analysis using window functions. `slide_*()` function family is useful for time series with regular observations (i.e. no missing time period) as in this example. If your time series appear to be irregular, `slide_index_*()` function family would be useful.
:::

Now, `forest_fire_ma` is a time series data frame with three series: original value `cnt`, 3-yr moving average `ma3`, and 6-yr moving average `ma6`. 


### Visualization

Covert this to a long form by calling `pivot_longer()`. The resulting time series data frame will have `key` that is a label of each series.

```{r}
forest_fire_ma_long <- 
  forest_fire_ma |>
  pivot_longer(c(cnt, ma3, ma6), names_to = "statistics")

forest_fire_ma_long
```


Visualize time series data. Call `autoplot()` with measurement variable name to draws line plot by each `key`.

```{r}
forest_fire_ma_long |> 
  autoplot(value)
```


## Example 1.2

### Load data

```{r}
household <- 
  read_excel("data/J01.xlsx", skip = 1) |> 
  rename(cnt = `#households`)
```

### Convert to time-series data

```{r}
household_ts <-
  household |> 
  as_tsibble(index = year)

household_ts
```


### Train/test split

```{r}
household_ts_train <-
  household_ts |> 
  filter(year <= 2014)

household_ts_test <-
  household_ts |> 
  filter(year > 2014)
```


### Double moving average with N = 4

```{r}
N <- 4

household_double_ma <-
  household_ts_train |> 
  mutate(
    ma = slider::slide_mean(cnt, before = N - 1, after = 0, complete = TRUE),
    ma_double = slider::slide_mean(ma, before = N - 1, after = 0, complete = TRUE)
  )

household_double_ma
```

Visualize results.

```{r}
household_double_ma |> 
  pivot_longer(c(cnt, ma, ma_double), names_to = "statistics") |> 
  autoplot(value)
```


### Estimate a slope

Take the latest moving average and double moving average.

```{r}
latest <- 
  household_double_ma |> 
  slice_tail(n = 1)

latest
```

Compute a slope

```{r}
b <- (latest$ma - latest$ma_double) / (N - 1) * 2
b
```


### Prediction

```{r}
household_ts_test |> 
  mutate(
    ma = latest$ma,
    ma_double = 2 * latest$ma - latest$ma_double + (year - latest$year) * b
  )
```


## Examples 1.3 - 1.4

### Load data

```{r}
patent <- 
  read_excel("data/J02.xlsx") |> 
  rename(cnt = `#patents`)
```

### Convert to time-series data

```{r}
patent_ts <-
  patent |> 
  as_tsibble(index = year)

patent_ts
```


### Train/test split

```{r}
patent_ts_train <-
  patent_ts |> 
  filter(year <= 2013)

patent_ts_test <- 
  patent_ts |> 
  filter(year > 2013)
```

### Example 1.3: Double exponential smoothing with $\alpha = 0.2$

#### Smoothing on training data

```{r}
alpha <- 0.2

ets_step <- function(x, y, alpha) {
  stopifnot(alpha >= 0)
  stopifnot(alpha <= 1)
  (1 - alpha) * x + alpha * y
}

patent_ets <- 
  patent_ts_train |> 
  mutate(
    es = accumulate(cnt, ets_step, alpha = alpha),
    es_double = accumulate(es, ets_step, alpha = alpha)
  )

patent_ets
```


Visualize the smoothing results.

```{r}
patent_ets |> 
  pivot_longer(c(cnt, es, es_double), names_to = "statistics") |> 
  autoplot(value)
```


#### Estimate coefficients

Take the last training time point.

```{r}
latest <- 
  patent_ets |> 
  slice_tail(n = 1)

n_train <- nrow(patent_ets)
latest_year <- latest$year
```

Estimate slope `b`.

```{r}
b <- alpha / (1 - alpha) * (latest$es - latest$es_double)
b
```

Estimate constant `c`.

```{r}
c <- 2 * latest$es - latest$es_double - b * n_train
c
```


#### Prediction

```{r}
patent_ts_test |> 
  mutate(
    forecast = c + b * (n_train + (year - latest_year)),
    forecast_error = cnt - forecast
  )
```


### Example 1.4: Holt's linear trend method

#### Smoothing on training data

Implement a function to update `L` and `b` in each iteration (i.e. each additional observation).

```{r}
holt_step <- function(param, x, alpha, beta) {
  L <- param$L
  b <- param$b

  L_new = alpha * x + (1 - alpha) * (L + b)
  b_new = beta * (L_new - L) + (1 - beta) * b

  res <- list(L = L_new, b = b_new)

  res
}
```

Set parameters for Holt smoothing.

```{r}
alpha <- 0.2
beta <- 0.2
```

Initialize `L` and `b` values.

```{r}
L1 <- patent_ts_train$cnt[1]
b1 <- patent_ts_train$cnt[2] - patent_ts_train$cnt[1]
```

Compute Holt smoothing over training time periods.

```{r}
patent_holt <- 
  patent_ts_train |> 
  mutate(
    es_double = accumulate(patent_ts_train$cnt[-1], holt_step, .init = list(L = L1, b = b1), alpha = alpha, beta = beta)
  ) |> 
  unnest_wider(es_double) |> 
  as_tsibble(index = year)

patent_holt
```

:::{.callout-note}
`unnest_wider()` returns `tibble` object, not `tsibble` object. sCall `as_tsibble()` to convert the results back to `tsibble` object.
:::

Visualize smoothing results.

```{r}
patent_holt |> 
  pivot_longer(c(cnt, L), names_to = "statistics") |> 
  autoplot(value)
```


#### Prediction

Obtain the latest value of `L` and `b`.

```{r}
latest <- 
  patent_holt |> 
  slice_tail(n = 1)

latest
```

Make a prediction on test data with linear trend assumption.

```{r}
patent_ts_test |> 
  mutate(
    forecast = latest$L + latest$b * (year - latest$year),
    forecast_error = cnt - forecast
  )
```


### Use `fable::ETS()`

Let us use `ETS()` from `{fable}`. The function name stands for Error-Trend-Seasonality. The approach is explained in [Forecasting: Principles and Practice](https://otexts.com/fpp3/holt.html) by Rob J Hyndman and George Athanasopoulos.

Smoothing parameters `alpha` within `ETS()` is the same to what we used in the previous section, but `beta` within `ETS()` is different from it of the previous chapter. In the following equations, $\beta^{*}$ is `beta` used in the previous section, while $\beta = \alpha \beta^{*}$ is `beta` in `ETS()`.

$$
\begin{eqnarray*}
l_t &=& \alpha y_t + (1 - \alpha)(l_{t - 1} + b_{t - 1})\\
b_t &=& \beta^{*}(l_t - l_{t - 1}) + (1 - \beta^{*})b_{t - 1}\\
    &=& \beta^{*}((y_t + (1 - \alpha)(l_{t - 1} + b_{t - 1})) - l_{t - 1}) + (1 - \beta^{*})b_{t - 1}\\
    &=& b_{t - 1} + \alpha \beta^{*} (y_t - l_{t - 1} - b_{t - 1})\\
    &=& b_{t - 1} + \beta \varepsilon_t
\end{eqnarray*}
$$

where $\beta = \alpha \beta^{*}$ and $\varepsilon_t = y_t - l_{t - 1} - b_{t - 1}$.

```{r}
alpha <- 0.2
beta <- 0.2

fit <- 
  patent_ts_train |> 
  model(AAN = ETS(cnt ~ error("A") + trend("A", alpha = alpha, beta = alpha * beta) + season("N")))

augment(fit)
```

Let us print estimated levels and slopes in training data

```{r}
fit$AAN[[1]]$fit$states
```

The results are still slightly different from previous section, because of different initialization of level and slope parameter value.

```{r}
fit |> 
  select(AAN) |> 
  report()
```

Let us create a forecast.

```{r}
fit |> 
  forecast(h = 3)
```


Now, call `ETS()` without specifying `alpha` and `beta` argument, so it finds the optimal value for fitting.

```{r}
fit_opt <- 
  patent_ts_train |> 
  model(AAN = ETS(cnt ~ error("A") + trend("A") + season("N")))

fitted(fit_opt)
```

Check the optimized smoothing paramters and initial states.

```{r}
fit_opt |> 
  select(AAN) |> 
  report()
```


Let us visualize estimated level, slope, and errors in the training data.

```{r}
fit_opt |> 
  components() |> 
  autoplot()
```

Make forecast for next three years.

```{r}
fit_opt |> 
  forecast(h = 3)
```

And visualize the forecast.

```{r}
fit_opt |> 
  forecast(h = 3) |> 
  autoplot(patent_ts_train)
```



## Example 1.5

### Load data

```{r}
gas_consumption <- read_excel("data/J03.xlsx", skip = 1) |> 
  fill(year, .direction = "down") |> 
  rename(consumption = comsumption) # fix typo in column name

gas_consumption
```

### Convert to tsibble object

```{r}
gas_consumption_ts <- 
  gas_consumption |> 
  mutate(year_month = make_yearmonth(year, month), .before = 1L) |> 
  select(!c(year, month)) |> 
  as_tsibble(index = year_month)

gas_consumption_ts
```


### Visualize seasonal pattern

Call `gg_season()` from `{feasts}` package to visualize seasonal pattern of the variable of interest.

```{r}
gas_consumption_ts |> 
  gg_season(consumption)
```

### Train/test data split

```{r}
gas_consumption_ts_train <- 
  gas_consumption_ts |> 
  filter(year_month < make_yearmonth(2017, 1))

gas_consumption_ts_test <- 
  gas_consumption_ts |> 
  filter(year_month >= make_yearmonth(2017, 1))
```


### Holt-Winters' multiplicative method

#### Initialization

Extract first 2 years data.

```{r}
m <- 12
r <- 2

gas_consumption_ts_init <-
  gas_consumption_ts_train |> 
  slice_head(n = m * r)
```

Compute an initial slope parameter value.

```{r}
b <- 
  gas_consumption_ts_init |> 
  mutate(slope = difference(consumption, lag = 12) / m) |> 
  pull(slope) |> 
  mean(na.rm= TRUE)

b
```

Compute initial seasonal factor values.

```{r}
s <-
  gas_consumption_ts_init |> 
  group_by(year(year_month)) |> 
  mutate(seasonal = consumption / mean(consumption)) |> 
  group_by(month(year_month)) |> 
  mutate(seasonal = mean(seasonal)) |> 
  ungroup() |> 
  select(year_month, consumption, seasonal) |> 
  slice_head(n = m) |> 
  pull(seasonal)

s
```

Compute initial level.

```{r}
l <- mean(gas_consumption_ts_init$consumption)
l
```


#### Smoothing on training data

```{r}
n_train <- nrow(gas_consumption_ts_train)
b_vec <- vector("numeric", length = m + n_train)
s_vec <- vector("numeric", length = m + n_train)
l_vec <- vector("numeric", length = m + n_train)
x_vec <- vector("numeric", length = m + n_train)

b_vec[m] <- b
s_vec[1:m] <- s
l_vec[m] <- l
x_vec[m + seq_len(n_train)] <- gas_consumption_ts_train$consumption
```

```{r}
alpha <- 0.1
beta <- 0.1
gamma <- 0.1

for (t in (m + 1):length(b_vec)) {
  l_vec[t] <- alpha * x_vec[t] / s_vec[t - m] + (1 - alpha) * (l_vec[t - 1] + b_vec[t - 1])
  b_vec[t] <- beta * (l_vec[t] - l_vec[t - 1]) + (1 - beta) * b_vec[t - 1]
  s_vec[t] <- gamma * (x_vec[t] / l_vec[t]) + (1 - gamma) * s_vec[t - m]
}
```


```{r}
gas_consumption_winters <- 
  gas_consumption_ts_train |> 
  mutate(
    l = l_vec[-seq_len(m)],
    b = b_vec[-seq_len(m)],
    s = s_vec[-seq_len(m)]
  )

gas_consumption_winters |> 
  tail(n = m)
```


#### Forecast

```{r}
l_latest <- 
  gas_consumption_winters |> 
  slice_tail(n = 1) |> 
  pull(l)

b_latest <- 
  gas_consumption_winters |> 
  slice_tail(n = 1) |> 
  pull(b)

s_latest <- 
  gas_consumption_winters |> 
  slice_tail(n = m) |> 
  pull(s)

month_latest <- 
  gas_consumption_winters |> 
  slice_tail(n = 1) |> 
  pull(year_month)
```


```{r}
gas_consumption_winters_forecast <- 
  gas_consumption_ts_test |> 
  mutate(
    k = year_month - month_latest,
    forecast = (l_latest + b_latest * k) * s_latest[(k - 1) %% m + 1],
    forecast_error = consumption - forecast
  )

gas_consumption_winters_forecast
```


### Use `fable::ETS()`

Holt-Winters' method can also be implemented in `ETS()` by specifying `season()` component. Here, set the multiplicative model that was specified in the previous section. `ETS()` for Holt-Winters' method is is explained in [Forecasting: Principles and Practice](https://otexts.com/fpp3/holt-winters.html) by Rob J Hyndman and George Athanasopoulos.


`gamma` parameter in `ETS()` is slightly different from `gamma` parameter in the previous section. Let $\gamma^{*}$ be the `gamma` in the previous section and $\gamma$ be the `gamma` in `ETS()`.

$$
\begin{eqnarray*}
s_t &=& \gamma^{*}\frac{y_t}{l_t} + (1 - \gamma^{*})s_{t - m}\\
    &=& \gamma\frac{y_t}{l_{t - 1} + b_{t - 1}} + (1 - \gamma)s_{t - m}
\end{eqnarray*}
$$

Let us train a model.

```{r}
alpha <- 0.1
beta <- 0.1
gamma <- 0.1

fit <- 
  gas_consumption_ts_train |> 
  model(MAM = ETS(consumption ~ error("M") + trend("A", alpha = alpha, beta = alpha * beta) + season("M", gamma = gamma)))

augment(fit)
```


See smoothing paramters.

```{r}
tidy(fit)
```


See fitness statistics.

```{r}
glance(fit)
```

Let us create a forecast for 1 year (i.e. 12 months).

```{r}
fit |> 
  forecast(h = 12)
```


Now, let the training internally optimize the smoothing parameters.

```{r}
fit_opt <- 
  gas_consumption_ts_train |> 
  model(MAM = ETS(consumption ~ error("M") + trend("A") + season("M")))
```

Check the optimized smoothing parameters.

```{r}
tidy(fit_opt)
```

Check the fitness statistics.

```{r}
glance(fit_opt)
```

And create forecast for next 1 year.

```{r}
fit_opt |> 
  forecast(h = 12)
```

Visualize the forecast as well as training data.

```{r}
fit_opt |> 
  forecast(h = 12) |> 
  autoplot(gas_consumption_ts_train)
```


## Example 1.6

### Load data

Load data from excel file, process it and convert it to be time series data frame `tsibble` object.

```{r}
electric_consumption <- 
  read_excel("data/J04.xlsx", skip = 1) |> 
  mutate(year = as.numeric(str_sub(`Y/Q`, 1, 4))) |> 
  fill(year, .direction = "down") |> 
  group_by(year) |> 
  mutate(quarter = row_number()) |> 
  ungroup() |> 
  mutate(year_quarter = make_yearquarter(year, quarter)) |> 
  select(year_quarter, consumption) |> 
  as_tsibble(index = year_quarter)

electric_consumption
```

Visualize the series.

```{r}
electric_consumption |> 
  autoplot(consumption)
```


### Train/test split

```{r}
electric_consumption_train <- 
  electric_consumption |> 
  filter(year(year_quarter) <= 2015)

electric_consumption_test <- 
  electric_consumption |> 
  filter(year(year_quarter) > 2015)
```

### Detrend training data

```{r}
m <- 4

detrended <-
  electric_consumption_train |> 
  mutate(
    trend = slider::slide_mean(consumption, before = 2, after = 1, complete = TRUE),
    trend = slider::slide_mean(trend, before = 0, after = 1, complete = TRUE),
    detrended = consumption - trend
  )

detrended
```

### Deseasonalize data

```{r}
seasonality <- 
  detrended |> 
  as_tibble() |> 
  mutate(quarter = quarter(year_quarter)) |> 
  group_by(quarter) |> 
  summarize(s = mean(detrended, na.rm = TRUE)) |> 
  mutate(s = s - mean(s)) |> 
  pull(s)

seasonality
```

```{r}
deseasonalized <- 
  detrended |> 
  mutate(
    seasonal = seasonality[quarter(year_quarter)],
    deseasonalized = consumption - seasonal,
    random = consumption - trend - seasonal
  )

deseasonalized
```


### Use `feasts::classical_decomposition()`

`classical_decomposition()` from `{feasts}` package is a short cut to decompose a series into trend, seasonality and random components. Find additional explanation from [Forecasting: Principles and Practice](https://otexts.com/fpp3/classical-decomposition.html) by Rob J Hyndman and George Athanasopoulos.

First, call `classical_decomposition()` inside `model()` function. The `model()` function returns `mable` object, which stands for "model table". To use the additive model, pass `type = "additive"` argument inside `classical_decomposition()`.

```{r}
electric_consumption_additive <- 
  electric_consumption_train |> 
  model(classical_decomposition(consumption, type = "additive"))

electric_consumption_additive
```

To see series of each component, call `components()`. This function returns `dable` object, which stands for "decomposition table".

`trend`, `seasonal`, and `random` columns represent trend, seasonality, and random component, respectively. `season_adjust` represents the deseasonalized series.

```{r}
electric_consumption_additive |> 
  components()
```

Visualize the components.

```{r}
electric_consumption_additive |> 
  components() |> 
  autoplot()
```



### Forecast

Use the deseasonalized series to produce a forecast.

```{r}
fit_poly <- 
  deseasonalized |> 
  mutate(t = row_number()) |> 
  lm(deseasonalized ~ t + I(t^2), data = _)

broom::tidy(fit_poly)
```

Create forecast for two years.

```{r}
electric_consumption_test <- 
  electric_consumption_test |> 
  mutate(t = nrow(electric_consumption_train) + row_number())
  
electric_consumption_test |> 
  mutate(
    trend = predict(fit_poly, newdata = electric_consumption_test),
    seasonality = seasonality[quarter(year_quarter)],
    forecast = trend + seasonality,
    forecast_error = consumption - forecast
  )
```


## Example 1.7

### Load data

Use the same data to previous example.

```{r}
electric_consumption <- 
  read_excel("data/J04.xlsx", skip = 1) |> 
  mutate(year = as.numeric(str_sub(`Y/Q`, 1, 4))) |> 
  fill(year, .direction = "down") |> 
  group_by(year) |> 
  mutate(quarter = row_number()) |> 
  ungroup() |> 
  mutate(year_quarter = make_yearquarter(year, quarter)) |> 
  select(year_quarter, consumption) |> 
  as_tsibble(index = year_quarter)

electric_consumption_train <- 
  electric_consumption |> 
  filter(year(year_quarter) <= 2015)

electric_consumption_test <- 
  electric_consumption |> 
  filter(year(year_quarter) > 2015)
```

### Detrend training data

```{r}
m <- 4

detrended <-
  electric_consumption_train |> 
  mutate(
    trend = slider::slide_mean(consumption, before = 2, after = 1, complete = TRUE),
    trend = slider::slide_mean(trend, before = 0, after = 1, complete = TRUE),
    detrended = consumption / trend
  )

detrended
```

### Deseasonalize data

```{r}
seasonality <- 
  detrended |> 
  as_tibble() |> 
  mutate(quarter = quarter(year_quarter)) |> 
  group_by(quarter) |> 
  summarize(s = mean(detrended, na.rm = TRUE)) |> 
  mutate(s = s / mean(s)) |> 
  pull(s)

seasonality
```

```{r}
deseasonalized <- 
  detrended |> 
  mutate(
    seasonal = seasonality[quarter(year_quarter)],
    deseasonalized = consumption / seasonal,
    random = consumption / (trend * seasonal)
  )

deseasonalized
```


### Use `feasts::classical_decomposition()`

Call `classical_decomposition()` with `type = "multiplicative"` argument.

```{r}
electric_consumption_multiplicative <- 
  electric_consumption_train |> 
  model(classical_decomposition(consumption, type = "multiplicative"))
```

```{r}
electric_consumption_multiplicative |> 
  components()
```

```{r}
electric_consumption_multiplicative |> 
  components() |> 
  autoplot()
```

### Forecast


```{r}
fit_poly <- 
  deseasonalized |> 
  mutate(t = row_number()) |> 
  lm(deseasonalized ~ t + I(t^2), data = _)

broom::tidy(fit_poly)
```

Create forecast for two years.

```{r}
electric_consumption_test <- 
  electric_consumption_test |> 
  mutate(t = nrow(electric_consumption_train) + row_number())
  
electric_consumption_test |> 
  mutate(
    trend = predict(fit_poly, newdata = electric_consumption_test),
    seasonality = seasonality[quarter(year_quarter)],
    forecast = trend * seasonality,
    forecast_error = consumption - forecast
  )
```


## Example 1.8

`{fabletools}` package that is automatically loaded along with `{fable}` provides forecast accuracy evaluation functions.

Let us evaluate 1-year forecast accuracy from Example 1.5. (Book example evaluated for 2 years, but here we will see only 1-year forecast.)

```{r}
MSE(gas_consumption_winters_forecast$forecast_error)
RMSE(gas_consumption_winters_forecast$forecast_error)
MAE(gas_consumption_winters_forecast$forecast_error)
MAPE(gas_consumption_winters_forecast$forecast_error, gas_consumption_winters_forecast$consumption)
```


## Example 1.9

### Load data

```{r}
patent_ts <- 
  read_excel("data/J02.xlsx") |> 
  rename(cnt = `#patents`) |> 
  as_tsibble(index = year)
```


### Time series cross-validation

To create one-step forecast in historical data, you need to set up time series cross-validation. Great explanation is provided in [Forecasting: Principles and Practice](https://otexts.com/fpp3/tscv.html) by Rob J Hyndman and George Athanasopoulos.

Call `stretch_tsibble()` to construct time series cross-validation that each training set is defined by `.id`, which is used as a key in the output `tsibble` object. Apply `filter(.id != max(.id))` to exclude the last set that entire data is used as training data.

```{r}
patent_cv <- 
  patent_ts |> 
  stretch_tsibble(.step = 1, .init = 7) |> 
  filter(.id != max(.id))

patent_cv
```

### Models

Let us use Holt's linear method with optimized initialization and smoothing parameters. As a result, it will return one model object per each cross-validation set.

```{r}
fit_cv <- 
  patent_cv |> 
  model(
    Holt = ETS(cnt ~ error("A") + trend("A") + season("N"))
  )

fit_cv
```


### Forecast

Call `forecast(h = 1)` to produce one-step forecast for each model in the `mable` object. As a result, it returns a `fable` object, forecast table, that each row represents each cross validation set and column `.mean` represents the point forecast.

```{r}
fc_cv <- 
  fit_cv |> 
  forecast(h = 1)

fc_cv
```


### Evaluation

Finally, evaluate accuracy by calling `accuracy()` function. It returns a data frame that a row represents a model and columns represent metrics.

```{r}
fc_cv |> 
  accuracy(patent_ts, measures = list(MSE = MSE, RMSE = RMSE, MAE = MAE, MAPE = MAPE))
```

The performance is better than the book example, because of optimal smoothing paramters and initialization.


### Multiple models

You can compare multiple models by adding more models in `model()`.

```{r}
fit_cv <- 
  patent_cv |> 
  model(
    Holt = ETS(cnt ~ error("A") + trend("A") + season("N")),
    Mean = MEAN(cnt),
    Naive = NAIVE(cnt),
    lm = TSLM(cnt ~ trend()),
    theta = THETA(cnt, method = "additive")
  )

fit_cv
```

Now, each combination of cross-validation set and forecast model shows one-step forecast.

```{r}
fc_cv <- 
  fit_cv |> 
  forecast(h = 1)

fc_cv
```

And you can compare models based on forecast accuracy metrics.

```{r}
fc_cv |> 
  accuracy(patent_ts, measures = list(MSE = MSE, RMSE = RMSE, MAE = MAE, MAPE = MAPE))
```
